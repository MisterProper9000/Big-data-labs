---
output:
  html_document: default
  pdf_document: default
---
### **Отчёт по работе "Большие данные: кластеризация и классификация"**

Грицаенко Никита, Жуков Александр, Сергеев Георгий, Митрофанова Алина, Чепулис Михаил, Плаксин Даниил

#### **Задача кластеризации**

**Задание**

Имеется набор данных о растениях Армориканской возвышенности (файл $plants.dat$). Требуется провести кластерный анализ данных методом k-медиан с целью их разбиения на $k$ групп со сходными признаками (Рассмотреть $k=2,3,4$). Сделать выводы.

**Теория**

Рассматривается множество объектов (ситуаций) $X$. 

Задано подмножество прецедентов $X^l={x_1,…,x_l }⊂X$, по каждому из которых собраны некоторые данные. Задана функция расстояния между объектами $\rho(x,x^{'})$, где $x,x^{'}∈X$.

Ставится задача разбить выборку $X$ на непересекающиеся подмножества – кластеры так, чтобы каждый кластер состоял из объектов, близких по метрике $\rho(x,x^{'})$, а объекты разных кластеров существенно отличались.

В данной работе использовалась евклидова метрика: $\rho(x,x^{'}) = \sqrt{\sum_{i=1}^{N}(x_i-x^{'}_i)^2}$

Решение проводится с помощью метода $k-medians$:

0) Начальный шаг: инициализация кластеров.
Выбирается произвольное множество точек $μ_i, i=1,...,k$, рассматриваемых как начальные центры кластеров.

1) Распределение векторов по кластерам. Каждому элементу выборки сопоставляется свой кластер путём сравнения расстояний от этого элемента до центров кластеров

2) Пересчет центров кластеров. Новый центр кластера - медиана среди всех элементов этого кластера

**Силуэт**

Обозначим через $a$ — среднее расстояние от данного объекта до объектов из того же кластера, через $b$ — среднее расстояние от данного объекта до объектов из ближайшего кластера (отличного от того, в котором лежит сам объект). Тогда силуэтом данного объекта называется величина:

$s = \displaystyle \frac{(b-a)}{max(a,b)}$

Силуэтом выборки называется средняя величина силуэта объектов данной выборки. Таким образом, силуэт показывает, насколько среднее расстояние до объектов своего кластера отличается от среднего расстояния до объектов других кластеров. Данная величина лежит в диапазоне $[-1, 1]$. 

Значения, близкие к $-1$, соответствуют плохим кластеризациям, значения, близкие к $0$, говорят о том, что кластеры пересекаются и накладываются друг на друга, значения, близкие к $1$, соответствуют "плотным" четко выделенным кластерам.

Таким образом, чем больше силуэт, тем более четко выделены кластеры, и они представляют собой компактные, плотно сгруппированные облака точек.

С помощью силуэта можно выбирать оптимальное число кластеров  (если оно заранее неизвестно) — выбирается число кластеров, максимизирующее значение силуэта.

**Данные**

Описание данных: 136 наблюдений, 31 переменная.
    
##### **Предобработка данных**


**Борьба c NA**

Предложим такой способ борьбы с NA (Not Available) - ячейки матрицы, помеченные как NA, заменим на медианное значение соответствующих ячеек в других прецедентах:

```r
data_dim = 31;
data_size = 136;

for(i in seq(1,data_dim)){
  med<-median(data.plants[,i],na.rm = TRUE)
  for(j in seq(1,data_size)){
    if(is.na(data.plants[j,i])){
      data.plants[j,i]=med
    }
  }
}
```

**Масштабирование**

```r
  dm = data.matrix(data.plants)
  for(i in 1:data_dim){
    dm[,i] = dm[,i]/norm( data.matrix(dm[,i]), type = "M")
  }
```


**Выделение нескольких признаков**

```r
  #Построим матрицу корреляции
  corr_dm = cor(dm, method = "pearson")

  #Посмотрим, где корреляция менее всего зависит от других переменных
  corr = 1:data_dim
  for(i in 1:data_dim){
    corr[i]=sum(abs(corr_dm[,i]))
  }

  variables_idx = c(5, 7, 30, 1, 6)
  dim = 5
  
  #Соберём новую матрицу, которая состоит из наименее коррелированных столбцов
  dm1 <- matrix(seq(1, 16), nrow = data_size, ncol = dim)
  for(i in 1 : dim){
    dm1[,i]=dm[,variables_idx[i]]
  }
```

**Зависимость силуэта от числа кластеров $k$**

![](plot_k.JPG)


##### **Поиск оптимального $k$**

**Графическое представление кластеров**

**$k=2$**

![](plot_k2.JPG)

**$k=3$**

![](plot_k3.JPG)

**$k=4$**

![](plot_k4.JPG)

**Вычисление оптимального $k$ с помощью библиотеки NbClust**
```r
NbClust(data = dm1, diss = NULL, distance = "euclidean", min.nc = 2, max.nc = 4, method = "median",index = "all")
```
                    Conclusion: According to the majority rule, the best number of clusters is 4

#### **Задача классификации**

Имеется множество объектов $X$, конечное множество ответов $Y$. 

Задана выборка $X^l={x_1,…,x_l }⊂X$ и множество известных ответов $y_i=a^*(x_i)$, вектор $x∈X$ – набор признаков, совокупность упорядоченных пар “объект-ответ” $(x_i,y_i)$ – обучающая выборка. 

Ставится задача построить решающее правило $a:X→Y$, которое приближало бы функцию $a^* (x)$ на всем множестве $X$ (построить алгоритм, классифицирующий произвольный объект из исходного множества).

#### **Задание**: 

Имеется таблица данных о качестве белых вин (Файл $winequality-white.csv$). Требуется методом деревьев по 90% данных построить классификатор и проверить его на 10% приведенных данных. Сделать выводы.

#### **Решение**:

Описание данных: 4898 наблюдений, 12 переменных.  Качество вина оценивается переменной $quality$, значения которой от 0 (плохое вино) до 10 (самое лучшее вино). Следующая гистограмма отображает исходные данные. 

```{r}
data.wine <- read.table('winequality-white.csv', 
                        sep=';', 
                        header=TRUE, 
                        na.strings="NA",
                        stringsAsFactors=T)

data.wine$quality <- as.factor(data.wine$quality)

data.wine[, -dim(data.wine)[2]] <- scale(data.wine[, -dim(data.wine)[2]])

element_samples <- summary(data.wine$quality)
barplot(element_samples, col = "peachpuff1")
```

![**Гистограмма распределения вин по качеству**](hist_origin_data.png)

Видно, что больше всего имеется сведений о вине среднего качества, а о вине низшего и высшего сорта известно мало. Вообще, для задачи классификации такое распределение исходных данных является очень плохим. Классификатор, построенный по этим данным, будет плохо работать.

Перемешаем данные, разделим их на две группы – тренировочную (90%) и тестовую (10%).
Получим следующие гистограммы для тренировочной и тестовой выборок.
```r
element_samples <- summary(data.train$quality)#apply(data.train[c(-10)] != 0, 2, sum)
barplot(element_samples, col = "peachpuff1")
abline(h = nrow(data.train), lty = 2, col = 2)
title(main = "Train set types number")

element_samples <- summary(data.test$quality)#apply(data.test[c(-10)] != 0, 2, sum)
barplot(element_samples, col = "peachpuff1")
abline(h = nrow(data.test), lty = 2, col = 2)
title(main = "Test set types number")
```

![**Гистограммы выборок**](hist_train&test_data.png)

Построим дерево решений при помощи `rpart`.
Полученное дерево решений представлено на иллюстрации ниже.
```r
tree <- rpart(quality ~., data.train)
rpart.plot(tree, 
           type=4,
           extra=101, 
           box.palette="GnBu",
           branch.lty=3, 
           shadow.col="gray", 
           nn=TRUE
)
predict.test <- predict(tree, data.test, type = "class")
predict.train <- predict(tree, data.train, type = "class")

result.test <- table(data.test$quality, predict.test)
result.train <- table(data.train$quality, predict.train)

accuracy.test <- sum(diag(result.test)) / sum(result.test)
accuracy.train <- sum(diag(result.train)) / sum(result.train)
```

![**Дерево классификатора**](tree_wines.png)

Полученное дерево охватывает не все категории вин из исходных данных, а только 5, 6 и 7. Из-за этого результат применения к тестовой выборке ожидается неудовлетворительным.

Применим дерево решений сначала к исходным данным, используя predict. Полученную классификацию вин сравниваем с исходной.

+ Для тренировочной выборки точность классификации составляет 0.5673469

+ Для тестовой выборки точность классификации составляет 0.5383394

Полученный результат следует признать плохим, поскольку дерево решений правильно классифицирует чуть больше 50% данных.

