---
output:
  word_document: default
  pdf_document: default
  html_document: default
---
### **Отчёт по работе "Большие данные: кластеризация и классификация"**

Грицаенко Никита, Жуков Александр, Сергеев Георгий, Митрофанова Алина, Чепулис Михаил, Плаксин Даниил

#### **Задача кластеризации**

**Задание**

Имеется набор данных о растениях Армориканской возвышенности (файл $plants.dat$). Требуется провести кластерный анализ данных методом k-медиан с целью их разбиения на $k$ групп со сходными признаками (Рассмотреть $k=2,3,4$). Сделать выводы.

**Данные**

Описание данных: 136 наблюдений, 31 переменная.
    
#### **Код программы**

```{r eval = TRUE}
library(cluster)
library(NbClust)
library(rpart.plot)

euc.dist <- function(x1, x2)
  sqrt(sum((x1 - x2) ^ 2))
```

**Борьба с NA**

Предложим такой способ борьбы с NA (Not Available) - ячейки матрицы, помеченные как NA, заменим на медианное значение соответствующих ячеек в других прецедентах:

```{r eval = TRUE}
removeNA <- function(matrix) {
  data_dim = ncol(matrix)
  data_size = nrow(matrix)
  M = matrix
  for (i in seq(1, data_dim)) {
    med <- median(M[, i], na.rm = TRUE)
    for (j in seq(1, data_size)) {
      if (is.na(M[j, i])) {
        M[j, i] = med
      }
    }
  }
  return(M)
}
```

**Масштабирование**

```{r eval = TRUE}
normalizeMatrix <- function(matrix) {
  data_dim = ncol(matrix)
  M = matrix
  for (i in seq(1, data_dim)) {
    M[, i] = M[, i] / norm(data.matrix(M[, i]), type = "M")
  }
  return(M)
}
```

**Кластеризация**

Кластеризуем с помощью функции `pam`. В качестве критерия кластеризации возвращаем среднеквадратическую ошибку разбиения:

```{r eval = TRUE}
clusterize <- function(matrix, num_of_clusters) {
  data_dim = ncol(matrix)
  data_size = nrow(matrix)
  
  cl <-
    pam(
      matrix,
      k = num_of_clusters,
      metric = "euclidean",
      stand = T,
      keep.diss = TRUE
    )
  
  centers = cl$medoids
  error = 0
  for (i in seq(1, data_size)) {
    min_dist = 1000000
    
    for (j in seq(1, num_of_clusters)) {
      dist = euc.dist(matrix[i, ], centers[j, ])
      
      if (dist < min_dist) {
        min_dist = dist
      }
    }
    error = error + min_dist ^ 2
  }
  
  return(sqrt(error))
}
```

**Считывание данных, их обработка**
```{r eval = TRUE}
# Read data in matrix M
path = 'data/plants.dat'
data.plants <- read.table(
  path,
  sep = ';',
  header = TRUE,
  na.strings = "NA",
  stringsAsFactors = T
)

data.plants$plant.name = NULL
M = data.matrix(data.plants)

data_dim = ncol(M)
data_size = nrow(M)

# Process data
M = removeNA(M)
M = normalizeMatrix(M)
```

**Выбор компонент для кластеризации**

```{r eval = TRUE}
#Построим матрицу корреляции
corr_dm = cor(M, method = "pearson")

#Посмотрим, где корреляция менее всего зависит от других переменных
corr = 1:data_dim
for (i in 1:data_dim) {
  corr[i] = sum(abs(corr_dm[, i]))
}

variables_idx = c(5, 7, 30, 1, 6)
dim = 5

 #Соберём новую матрицу, которая состоит из наименее коррелированных столбцов
newM <- matrix(0, nrow = data_size, ncol = dim)

for (i in 1:dim) {
  newM[, i] = M[, variables_idx[i]]
}
```

**Кластеризация**
```{r eval = TRUE}
# Trying to clustering
min_num_of_clusters = 2
max_num_of_clusters = 4

error_vec = matrix(0, nrow = 1, ncol = 3)
i = 1

for (k in seq(min_num_of_clusters, max_num_of_clusters)) {
  error_vec[i] = clusterize(newM, k)
  i = i + 1
}

plot(min_num_of_clusters:max_num_of_clusters,
     error_vec,
     "l",
     main = "Sqr error for diffirent N of clusters",
     xlab = "num of clusters",
     ylab = "square error")
```

Видим, что по критерию среднеквадратичной ошибки наилучшее число кластеров: 4

**Поиск оптимального числа кластеров с помощью NbClust**

С помощью пакета NbClust можно найти оптимальную схему объединения в кластеры, используя 30 индексов. При этом происходит перебор различных комбинаций числа групп, метрик дистанции и методов кластеризации. Вывод об оптимальном числе классов делается с помощью голосования: берется то число кластеров, за которое "проголосовало" большинство критериев.

```{r eval = FALSE}
# Finding optimal N of clusters
NbClust(
  data = newM,
  diss = NULL,
  distance = "euclidean",
  min.nc = min_num_of_clusters,
  max.nc = max_num_of_clusters,
  method = "median",
  index = "all"
)
```

```
* Among all indices:                                                
* 9 proposed 2 as the best number of clusters 
* 4 proposed 3 as the best number of clusters 
* 11 proposed 4 as the best number of clusters 

                   ***** Conclusion *****                            
 
* According to the majority rule, the best number of clusters is  4 
```

***Видим, что наш результат совпал с результатом NbClust.***

### **Задача классификации**

Имеется множество объектов $X$, конечное множество ответов $Y$. 

Задана выборка $X^l={x_1,…,x_l }⊂X$ и множество известных ответов $y_i=a^*(x_i)$, вектор $x∈X$ – набор признаков, совокупность упорядоченных пар “объект-ответ” $(x_i,y_i)$ – обучающая выборка. 

Ставится задача построить решающее правило $a:X→Y$, которое приближало бы функцию $a^* (x)$ на всем множестве $X$ (построить алгоритм, классифицирующий произвольный объект из исходного множества).

#### **Задание**: 

Имеется таблица данных о качестве белых вин (Файл $winequality-white.csv$). Требуется методом деревьев по 90% данных построить классификатор и проверить его на 10% приведенных данных. Сделать выводы.

#### **Данные**:

Описание данных: 4898 наблюдений, 12 переменных.  Качество вина оценивается переменной $quality$, значения которой от 0 (плохое вино) до 10 (самое лучшее вино). Следующая гистограмма отображает исходные данные. 


#### **Код программы**

**Библиотеки и функции, необходимые для работы программы**
+ `drawHistogram` - создает и отображает гистограмму с заголовком `title` для таблицы `wines`. В качестве критерия используется колонка с именем `quality`. Над каждым столбцом гистограммы отображается соответствующее числовое значение.
+ `makeDataSets` - перемешивает данные(строки) таблицы `wines` случайным образом и создает два набора, содержащие 90% и 10% данных соответственно. Возвращаемое значение - список из двух наборов данных. Случайный закон инициализируется константным значением `seed`, чтобы результаты воспроизводились от опыта к опыту.
+ `testModel` - производит тестирование модели `tree` на наборах `test_data` и `train_data.` Предполагается, что обучение модели производится с помощью набора `train_data.` 
+ `combineClasses` - изменяет столбец quality, являющийся фактором таблицы wines. Комбинирует уровни факторов '3', '4', '5', а также '7', '8', '9' в уровни 'Low' и 'High'. Уровень '6' переименовывается в 'Medium'.
**Запустите этот фрагмент кода для корректной работы следующих фрагментов.**
```{r eval = TRUE}
library(repr)
library(maptree)
library(tree)
library(caret)
library(forcats)
library(dplyr)

drawHistogram <- function(wines, title){
  options(repr.plot.width=6, repr.plot.height=6)
  element_samples <- summary(wines$quality)
  midpoints <- barplot(element_samples, col = "peachpuff1", main = title)
  abline(h = nrow(wines), lty = 2, col = 2)
  num_elements <- dim(data.wine)[1]
  num_types <- length(element_samples)
  y_coord <- element_samples
  max_val <- max(element_samples)
  for (j in seq(1, num_types)) {
    if (element_samples[j] / max_val > 0.9){
      y_coord[j] <- y_coord[j] - 0.1 * max_val 
    }
    else{
      y_coord[j] <- y_coord[j] + 0.05 * max_val
    }
        
  }
  text(midpoints, y_coord, labels=element_samples)
}

makeDataSets <- function(wines){
  set.seed(12345) #constant seed for result reproducibility
  n <- dim(wines)[1]
  data <- wines[order(runif(n)),]
  n.train <- as.integer(0.8 * n)
  data.train <- data[1:n.train,]
  data.test <- data[(n.train + 1):n,]
  return (list(train = data.train, test = data.test))
}

testModel <- function(tree, test_data, train_data){
  
  predict.test <- predict(tree, test_data, type = "class")
  predict.train <- predict(tree, train_data, type = "class")
  
  result.test <- table(test_data$quality, predict.test)
  result.train <- table(train_data$quality, predict.train)
  
  accuracy.test <- sum(diag(result.test)) / sum(result.test)
  accuracy.train <- sum(diag(result.train)) / sum(result.train)
  print("test data prediction accuracy: ")
  print(accuracy.test)
  print("train data prediction accuracy:")
  print(accuracy.train)
  
  #confusion matrix
  caret::confusionMatrix(test_data$quality, predict.test)
  
}


combineClasses <- function(wines){
  df = wines
  df %>%
    mutate(quality = fct_collapse(df$quality,
                                  Low = c("3","4","5"),
                                  Medium = c("6"),
                                  High = c("7","8","9")))->x
  return(x)
}
```
**Считывание данных и предобработка**
```{r eval = TRUE}
# Read data
data.wine <- read.table('data/winequality-white.csv', 
                        sep=';', 
                        header=TRUE, 
                        na.strings="NA",
                        stringsAsFactors=T)

# Make 'quality' vector to be factor
data.wine$quality <- as.factor(data.wine$quality)

# Remove 'NA' rows from table. 
# N.B. In file 'data/winequality-white.csv' there is no NA values, so this line is irrelevant
data.wine <- data.wine[complete.cases(data.wine),]

# Scale all columns in table except 'quality', which is factor column
data.wine[, -dim(data.wine)[2]] <- scale(data.wine[, -dim(data.wine)[2]])
```

**Гистограмма распределения вин по качеству**
```{r eval = TRUE}
drawHistogram(data.wine, "Гистограмма вин по качеству")
```

Видно, что больше всего имеется сведений о вине среднего качества, а о вине низшего и высшего сорта известно мало. Вообще, для задачи классификации такое распределение исходных данных является очень плохим. Классификатор, построенный по этим данным, будет плохо работать.


**Разделение данных на тренировочную и тестовую выборки**

Перемешаем данные, разделим их на две группы – тренировочную (90%) и тестовую (10%).

```{r eval = TRUE}

data <- makeDataSets(data.wine)

```

Получим следующие гистограммы для тренировочной и тестовой выборок.
```{r eval = TRUE}

drawHistogram(data$train, "Тренировочная выборка")

```
```{r eval = TRUE}

drawHistogram(data$test, "Тестовая выборка")

```

**Построение модели классификации**
Построим дерево решений при помощи `tree`. Проверим его на выборках, а также построим матрицу сопряженности для построенной модели.

```{r eval = TRUE}

wine.tree <- tree(quality ~., data$train)

testModel(wine.tree, data$test, data$train)

```

Полученное дерево решений представлено на иллюстрации ниже.
```{r eval = TRUE}
options(repr.plot.width = 14, repr.plot.height = 10)
draw.tree(wine.tree, cex = 0.75)
```

Построим дерево решений при помощи `rpart`. Проверим его на выборках, а также построим матрицу сопряженности для построенной модели.

```{r eval = TRUE}
wine.rpart <- rpart(quality ~., data$train)
testModel(wine.rpart, data$test, data$train)

```

Полученное дерево решений представлено на иллюстрации ниже.
```{r eval = TRUE}
draw.tree(wine.rpart, cex = 0.75)
rpart.plot(wine.rpart, 
           type=4,
           extra=101, 
           box.palette="GnBu",
           branch.lty=3, 
           shadow.col="gray", 
           nn=TRUE,
           roundint = FALSE
)
```

Полученные деревья охватывают не все категории вин из исходных данных, а только 5, 6 и 7. Из-за этого результат применения к тестовой выборке оказывается неудовлетворительным.

Полученный результат следует признать плохим, поскольку дерево решений правильно классифицирует чуть больше 50% данных.

**Улучшение распознавания путем слияния классов качества вин**

Попробуем объединить классы вин 3,4,5 и 7,8,9, чтобы получить более равномерное распределение, чем ранее. Гистограмма данных с новым фактором представлена ниже.
```{r eval = TRUE}
x <- combineClasses(data.wine)
drawHistogram(x, "Объединенные классы вин")
```

**Построение модели и тестирование**
Построим модель с помощью метода tree. Проверим на тестовой и тренировочной выборке, построим матрицу сопряженности.
```{r eval = TRUE}
xdata <- makeDataSets(x)
x.tree <- tree(quality ~., xdata$train)

testModel(x.tree, xdata$test, xdata$train)
```
Изображение дерева представлено ниже.
```{r eval = TRUE}
options(repr.plot.width = 14, repr.plot.height = 10)
draw.tree(x.tree, cex = 0.75)
```
Построим модель с помощью метода rpart.  Проверим на тестовой и тренировочной выборке, построим матрицу сопряженности.
```{r eval = TRUE}
x.rpart <- rpart(quality ~., xdata$train)
testModel(x.rpart, xdata$test, xdata$train)
```
Изображение дерева представлено ниже.
```{r eval = TRUE}
draw.tree(x.rpart, cex = 0.75)
rpart.plot(x.rpart, 
           type=4,
           extra=101, 
           box.palette="GnBu",
           branch.lty=3, 
           shadow.col="gray", 
           nn=TRUE,
           roundint = FALSE
)
```
####**Заключение**    
По итогам проведенного объединения классов получено незначительное улучшение на 3-4% при распознавании как тестовой, так и тренировочной выборок.
Метод `rpart` даёт результаты чуть лучше, чем метод `tree`. Кроме того, в библиотеке `rpart` есть метод `rpart.plot` для удобного и наглядного изображения дерева решений.